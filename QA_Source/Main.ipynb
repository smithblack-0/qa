{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "453c8395",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import supertransformerlib\n",
    "\n",
    "\n",
    "??set.remove"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the Dataloaders\n",
    "\n",
    "This section contains two important logical\n",
    "tasks. These are chunking the source data,\n",
    "and creating the dataloaders.\n",
    "\n",
    "The pipeline parameters, such as batch size, are also\n",
    "defined in this section.\n",
    "\n",
    "## Chunking\n",
    "\n",
    "Microsoft has developed the wonderful infinibatch library, which is capable\n",
    "of performing several big data tasks. First it is capable of loading from a\n",
    "diverse set of files, known as chunks, many terrabytes large. Second, it has\n",
    "a \"yield\" based pipeline for computing things only when needed, and can\n",
    "do dynamic batching.\n",
    "\n",
    "While technically I am not required to chunk a dataset this small, it is\n",
    "great practice for how to use this powerful tool. As a result, the first section\n",
    "breaks the train file up into a set of train, test, and validation folders,\n",
    "each of which contains a single chunk of topics.\n",
    "\n",
    "## Dataloaders\n",
    "\n",
    "Once chunked, the information is then loaded in a loader pipeline. This\n",
    "performs the following magic. The input is loaded by the chunk loader, which\n",
    "opens N chunks at once and draws examples from them. This input consists of\n",
    "a diverse set of information arranged by topic.\n",
    "\n",
    "\n",
    "The three primary features which will be available to the model are\n",
    "\n",
    "* topic\n",
    "* passage\n",
    "* question\n",
    "\n",
    "These are encoded as char representations\n",
    "\n",
    "todo: finish"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**parameters**\n",
    "\n",
    "Pipeline parameters are located here. Must be run."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "#Path and chunking\n",
    "source_file = './source/train-v2.0.json'\n",
    "chunk_dir = './Chunks'\n",
    "train_test_val_split = [100, 1, 1]\n",
    "\n",
    "#Loader parameters.\n",
    "loader_buffer = 1000\n",
    "readahead_buffer = 1000\n",
    "prefetch_buffer = 20\n",
    "batch_size = 16"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Chunking**\n",
    "\n",
    "This will split the source up into chunks. Optional"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunking' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mchunking\u001B[49m\u001B[38;5;241m.\u001B[39mchunk(source_file, chunk_dir)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchunked\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'chunking' is not defined"
     ]
    }
   ],
   "source": [
    "chunking.chunk(source_file, chunk_dir)\n",
    "print(\"chunked\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Loaders**\n",
    "\n",
    "This now makes the data loaders."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def my_pipeline(subfolder, infinite: bool):\n",
    "    path = chunk_dir + '/' + subfolder\n",
    "    pipe = pipeline.start_pipeline(path, loader_buffer,\n",
    "                                   infinite, readahead_buffer,\n",
    "                                   prefetch_buffer\n",
    "                                   )\n",
    "    pipe = pipeline.clean_text(pipe)\n",
    "    pipe = pipeline.convert_to_tensors(pipe)\n",
    "\n",
    "\n",
    "train_loader = my_pipeline(\"Train\", True)\n",
    "test_loader = my_pipeline(\"Test\", False)\n",
    "val_loader = my_pipeline(\"Val\", False)\n",
    "\n",
    "for i in range(10):\n",
    "    features = next(train_loader)\n",
    "    print(features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}