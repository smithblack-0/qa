{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "This section will be related to an exploration of the huggingface\n",
    "tokenization word\n",
    "\n",
    "## Regex\n",
    "\n",
    "Notably, a lot of different features use regex. the regex feature\n",
    "is located in tokenizers as \"from tokenizers import Regex\"\n",
    "\n",
    "## Normalization\n",
    "\n",
    "Normalization, and normalizers, are functions\n",
    "responsible for translating and handling the cleanup\n",
    "of strings. They are responsible for such activities\n",
    "as separating concept-level languages, translating unicode\n",
    "characters into more meaningful forms, and other sorts\n",
    "of text refinement.\n",
    "\n",
    "Anything involving \"cleaning\" the text, in other words,\n",
    "should go here."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tokenizers import normalizers\n",
    "\n",
    "norms = [\n",
    "    normalizers.NFKD(),\n",
    "    normalizers.Strip(),\n",
    "    normalizers.StripAccents(),\n",
    "]\n",
    "normalizer = normalizers.Sequence(norms)\n",
    "normalizer.normalize_str(\"this is a test\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-Tokenization\n",
    "\n",
    "Pre-Tokenization is the process of splitting up the string into\n",
    "subcomponents which we will then be using for actual tokenization. It performs the \"split into arrays\" portion of tokenization.\n",
    "\n",
    "Importantly, pretokenization can be applied in sequence with absolutely no problem.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "#Standard tokenization\n",
    "pretoksequence = [\n",
    "    pre_tokenizers.Punctuation(),\n",
    "    pre_tokenizers.Whitespace(),\n",
    "    pre_tokenizers.Digits(individual_digits=True),\n",
    "\n",
    "]\n",
    "\n",
    "pretokenizer = pre_tokenizers.Sequence(pretoksequence)\n",
    "item = pretokenizer.pre_tokenize_str(\"Hey, let's see how this does: 123445\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "[('H', (0, 1)),\n ('e', (1, 2)),\n ('y', (2, 3)),\n (',', (3, 4)),\n (' ', (4, 5)),\n ('l', (5, 6)),\n ('e', (6, 7)),\n ('t', (7, 8)),\n (\"'\", (8, 9)),\n ('s', (9, 10)),\n (' ', (10, 11)),\n ('s', (11, 12)),\n ('e', (12, 13)),\n ('e', (13, 14)),\n (' ', (14, 15)),\n ('h', (15, 16)),\n ('o', (16, 17)),\n ('w', (17, 18)),\n (' ', (18, 19)),\n ('t', (19, 20)),\n ('h', (20, 21)),\n ('i', (21, 22)),\n ('s', (22, 23)),\n (' ', (23, 24)),\n ('d', (24, 25)),\n ('o', (25, 26)),\n ('e', (26, 27)),\n ('s', (27, 28)),\n (':', (28, 29)),\n (' ', (29, 30)),\n ('1', (30, 31)),\n ('2', (31, 32)),\n ('3', (32, 33)),\n ('4', (33, 34)),\n ('4', (34, 35)),\n ('5', (35, 36))]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Custom char pretokenizer. Note the usage of regex.\n",
    "#\n",
    "# The core library is written in rust, so this is faster\n",
    "#than a custom class.\n",
    "from tokenizers import Regex\n",
    "\n",
    "pattern = Regex('.')\n",
    "pretokenizer=  pre_tokenizers.Split(pattern, 'isolated')\n",
    "pretokenizer.pre_tokenize_str(\"Hey, let's see how this does: 123445\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "Any kind of tokenization requires training. At some point along the way, the tokenization system must construct a vocabulary of some kind, such that words can be turned into tokens and vice versa. The degree of training, however, varies.\n",
    "\n",
    "## WordLevel\n",
    "\n",
    "The most simple variety of tokenizer is perhaps the word level tokenizer. It functions by means of simply mapping the incoming tokens onto a vocabulary, whose size is limited. Unknown words must, by virtue of being unknown, be represented in terms of a unique unknown token, and the model makes its best guess as to the content of the token.\n",
    "\n",
    "## BPE, WordPiece\n",
    "\n",
    "Both of these build up their vocabulary in much the same way.\n",
    "\n",
    "Figure out all the individual characters which are within the input stream. Take those characters, and then look at the text. Figure out what groups of characters, when merged together, would reduce the overall token count. Repeat until vocabulary is within acceptable limits.\n",
    "\n",
    "Byte Pair Encoding and WordPiece are both excellent tokenization agorithms which require training. They look at the entire vocabulary, and attempt to merge it down to a particular size.\n",
    "\n",
    "## Unigram\n",
    "\n",
    "Unigram does something else. It starts by initializing a complete vocabulary, and then prunes down from here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "% md\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extra\n",
    "\n",
    "A few prebuilt, additional tokenizers are available. They still require training\n",
    "\n",
    "## Sentencepiece.\n",
    "\n",
    "Sentencepiece works well when a language does not have a clear distinguishment between words, as in, for example, chinese. It treats an entire sentence as a token, and then attempts to break it apart into bits\n",
    "\n",
    "\n",
    "The two varieties are\n",
    "\n",
    "* SentencePieceUnigramTokenizer\n",
    "* SentencePieceBPETokenizer,\n",
    "\n",
    "Due to the way imports are run, pycharm, and likely other ide's, do not detect thes eas valid imports before running\n",
    "\n",
    "## Extra\n",
    "\n",
    "A few extra tokenizers are available, but perhaps not clearly listed\n",
    "\n",
    "* CharBPETokenizer\n",
    "* ByteLevelBPETokenizer\n",
    "* BertWordPieceTokenizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a basic pretrained tokenizer.\n",
    "\n",
    "Training a tokenizer takes quite a bit of time, but is nonetheless fairly straightforward. Make a dataset of the text content, shove it through an iterator, and let it work from there. One additional item of note: It is likely beneficial to start from a premade tokenizer which is reasonably close to your application.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Test', 'this', 'token', '##ization', '119', '##0']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "\n",
    "## Load a premade tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.tokenize(\"Test this tokenization 1190\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 2662/2662 [00:00<00:00, 14921.24it/s]\n",
      "Using custom data configuration train-novels-042faceeba55f51c\n",
      "Reusing dataset text (C:\\Users\\chris\\.cache\\huggingface\\datasets\\text\\train-novels-042faceeba55f51c\\0.0.0\\acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08)\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 14747362\n    })\n})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.text.all import get_text_files\n",
    "import datasets\n",
    "\n",
    "\n",
    "#Update it to be more suitable to the particular application using more training.\n",
    "#Make sure to use this to extract data. Significantly faster than raw python\n",
    "\n",
    "src = r'C:\\Users\\chris\\PycharmProjects\\qa\\Data\\lambada-dataset\\train-novels'\n",
    "files = [str(item) for item in get_text_files(src)]\n",
    "ds = datasets.load_dataset(src, '.txt', data_files=files )\n",
    "ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': [\" prism story of ci prism -lrb- story of ci -rrb- copyright 2010 rachel moschell published by rachel moschell at smashwords third edition the prism table of contents 0 gilded 1 purple 2 gaudy gold 3 electric blue 4 mocha 5 aquamarine 6 emerald 7 white plaster 8 scarlet 9 silver 10 midnight blue 11 coffee 12 hazel 13 olive green 14 beet red 15 pale 16 canary yellow 17 brick red 18 dark 19 transparent 20 red white and blue 21 cinnamon 22 sickly pink 23 bittersweet 24 sea green 25 crimson 26 pale blue 27 sapphire 28 fiery 29 turquoise 30 blond 31 plaid 32 white 33 grape 34 lilac preview of reverb -lrb- story of ci 2 -rrb- 0 gilded the silvery branches of the molle tree whispered in the shade , sprinkling soft leaves in the dirt of the boy 's path . \",\n  ' the sky shone sapphire behind the lacy branches , and the only other life along this back road were two tawny cows grazing in the silver white grass of the ditch . ',\n  ' behind them ran the same crumbling adobe wall the boy saw every day as he walked this way out of town for the afternoon session at the christian school his parents sent him to . ',\n  ' today was tuesday , and that meant bible class . ',\n  ' in one coffee-colored hand , the boy clutched his fat black bible with gilded letters , a present from his parents on his last birthday , fourteen . ',\n  ' he shuffled slowly along the road , feeling the bible burn into his fingertips , trying not to free the furious tears scalding the back of his eyes . ',\n  ' he should leave the bible here in the ditch by the cows . ',\n  ' because now the boy knew : none of it really mattered . ',\n  ' oh , he was quite sure there was justice and virtue in the world , and he intended to find it . ',\n  ' but it was not here . ']}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training = ds['train']\n",
    "training.shuffle()\n",
    "training[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batches: 10000\n",
      "batch 0 of 10000\n",
      "batch 1000 of 10000\n",
      "batch 2000 of 10000\n",
      "batch 3000 of 10000\n",
      "batch 4000 of 10000\n",
      "batch 5000 of 10000\n",
      "batch 6000 of 10000\n",
      "batch 7000 of 10000\n",
      "batch 8000 of 10000\n",
      "batch 9000 of 10000\n",
      "batch 10000 of 10000\n"
     ]
    }
   ],
   "source": [
    "#Create a generator to load batches of data.\n",
    "\n",
    "batch_size = 128\n",
    "def loader(ds_split, max_batches = 10000):\n",
    "    total_batches = min(len(ds_split) // batch_size + 1, max_batches)\n",
    "    batch_count = 0\n",
    "    print(\"total batches: %s\" % total_batches)\n",
    "    collection = []\n",
    "\n",
    "    for i, instance in enumerate(ds_split):\n",
    "        collection.append(instance['text'])\n",
    "        if len(collection) > batch_size:\n",
    "            if batch_count % 1000 == 0:\n",
    "                print(\"batch %s of %s\" % (batch_count, total_batches))\n",
    "            yield collection\n",
    "            batch_count += 1\n",
    "            collection = []\n",
    "        if batch_count > max_batches:\n",
    "            break\n",
    "    yield collection\n",
    "\n",
    "#Train the data based on the vocab\n",
    "ds_loader = loader(training)\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(ds_loader, vocab_size=60000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "('my_example_pretrained\\\\tokenizer_config.json',\n 'my_example_pretrained\\\\special_tokens_map.json',\n 'my_example_pretrained\\\\vocab.txt',\n 'my_example_pretrained\\\\added_tokens.json',\n 'my_example_pretrained\\\\tokenizer.json')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Once trained, you want to save it so you can use it again later\n",
    "new_tokenizer.save_pretrained(\"my_example_pretrained\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " they were in position , ten feet away from their home ship . \n"
     ]
    },
    {
     "data": {
      "text/plain": "['they',\n 'were',\n 'in',\n 'position',\n ',',\n 'ten',\n 'feet',\n 'away',\n 'from',\n 'their',\n 'home',\n 'ship',\n '.']"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And it can now be loaded from here!\n",
    "import random\n",
    "my_tokenizer = transformers.AutoTokenizer.from_pretrained(\"my_example_pretrained\")\n",
    "example = training[random.randint(0, 1000000)]['text']\n",
    "print(example)\n",
    "my_tokenizer.tokenize(example)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training tokenizers completely from scratch: A comparison\n",
    "\n",
    "Sometimes, it might become necessary to train a tokenizer completely from scratch. In which case, it is possible to initialize a raw tokenizer, then go use the train iterator.\n",
    "\n",
    "Lets train a few different tokenizers from scratch. We will keep track of the time for comparison, and only train them briefly. We will train, basically, all of them. These are all basically predefined pipelines.\n",
    "\n",
    "* SentencePieceBPETokenizer\n",
    "*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "Tokenizer(vocabulary_size=0, model=SentencePieceBPE, unk_token=<unk>, replacement=▁, add_prefix_space=True, dropout=None)"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run imports\n",
    "import datasets\n",
    "import timeit\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "\n",
    "#Get tokenizers\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tokenizers import CharBPETokenizer\n",
    "from tokenizers import SentencePieceUnigramTokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "SentencePieceBPETokenizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "\"<class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>\""
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lambada (C:\\Users\\chris\\.cache\\huggingface\\datasets\\lambada\\plain_text\\1.1.0\\e32d76a7236c9ebb30099bc73d677c3acf32ddffb411836fe9ffc091ad3f3bec)\n",
      "100%|██████████| 3/3 [00:00<00:00, 25.88it/s]\n"
     ]
    }
   ],
   "source": [
    "#Configure our dataset\n",
    "\n",
    "ds = datasets.load_dataset(\"lambada\")\n",
    "training = ds['train']\n",
    "\n",
    "def filter_length(x, min_length, max_length):\n",
    "    return min_length < len(x['text']) < max_length\n",
    "\n",
    "\n",
    "def yield_batches(dataset: datasets.Dataset,\n",
    "                 min_length: int,\n",
    "                 max_length:int,\n",
    "                 batch_size: int,\n",
    "                 num_batches: int):\n",
    "    \"\"\"\n",
    "\n",
    "    Gets the data for training\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = dataset.filter(partial(filter_length, min_length=min_length, max_length=max_length))\n",
    "    batch_collection = []\n",
    "    batch_num = 0\n",
    "    for item in dataset:\n",
    "        batch_collection.append(item['text'])\n",
    "        if len(batch_collection) >= batch_size:\n",
    "            if batch_num % (num_batches//10) == 0:\n",
    "                print(\"batch %s of %s\" % (batch_num, num_batches))\n",
    "\n",
    "            yield batch_collection\n",
    "            if batch_num >= num_batches:\n",
    "                break\n",
    "            batch_collection = []\n",
    "            batch_num += 1\n",
    "    yield batch_collection\n",
    "\n",
    "\n",
    "def train_tokenizer(dset, n_batches, batch_size, tokenizer, lengths):\n",
    "    print(\"training tokenizer: number %s, batch %s, token %s, lengths %s\"\n",
    "          % (n_batches, batch_size, str(tokenizer), lengths))\n",
    "    statistics = {\n",
    "        'n_batches' : n_batches,\n",
    "        'batch_size' : batch_size,\n",
    "        'tokenizer' : str(tokenizer)}\n",
    "\n",
    "    train_batches = yield_batches(dset, lengths[0], lengths[1], batch_size, n_batches)\n",
    "    tokenizer = tokenizer()\n",
    "    start = timer()\n",
    "\n",
    "    #Note it is \"Train From Iterator\" not \"Train_new_from_iterator\"\n",
    "    tokenizer = tokenizer.train_from_iterator(train_batches)\n",
    "    end = timer()\n",
    "    print(\"elapsed %s\" % (end-start))\n",
    "    statistics['elapsed'] = end-start\n",
    "    statistics['tkn'] = tokenizer\n",
    "    return statistics\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.85ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.6916866999963531\n",
      "(100, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2769699999989825\n",
      "(100, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.3438907000017934\n",
      "(100, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.2478760000012699\n",
      "(100, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2954339999996591\n",
      "(100, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.356816700004856\n",
      "(100, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.3084346000032383\n",
      "(100, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2705461000005016\n",
      "(100, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.353767799999332\n",
      "(100, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.3020074000014574\n",
      "(100, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2809663999942131\n",
      "(100, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.49ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.2934448999949382\n",
      "(100, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.2777342000044882\n",
      "(100, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.48ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2510975999975926\n",
      "(100, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.2936815000066417\n",
      "(100, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2979200999980094\n",
      "(100, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3050847999984398\n",
      "(100, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2955463999969652\n",
      "(100, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.49ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2201088999936474\n",
      "(100, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3380495000019437\n",
      "(100, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2862878000014462\n",
      "(100, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2521962999962852\n",
      "(100, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.330471100001887\n",
      "(100, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3325036999958684\n",
      "(100, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2884132000035606\n",
      "(100, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.45ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2724520999981905\n",
      "(100, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3571018000002368\n",
      "(100, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.49ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2159792000020389\n",
      "(100, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.48ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2472769000014523\n",
      "(100, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.340424100002565\n",
      "(100, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2701121000063722\n",
      "(100, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.260712200004491\n",
      "(100, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3175147000001743\n",
      "(100, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.37ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2809833999999682\n",
      "(100, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2536536999978125\n",
      "(100, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.296337699997821\n",
      "(100, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3572767000005115\n",
      "(100, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3498147999998764\n",
      "(100, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.367449499994109\n",
      "(100, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3058919999966747\n",
      "(100, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.307646900000691\n",
      "(100, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3384625999970012\n",
      "(100, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.30ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3132393999985652\n",
      "(100, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.278508299998066\n",
      "(100, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3063079999992624\n",
      "(100, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2898939999940922\n",
      "(100, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.10ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.93993380000029\n",
      "(100, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.16ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4736048999984632\n",
      "(100, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3292274000050384\n",
      "(100, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.358046800000011\n",
      "(100, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.22ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4136472000027425\n",
      "(100, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2757873000009567\n",
      "(100, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.362846999996691\n",
      "(100, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.20ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4509859000027063\n",
      "(100, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2730617999986862\n",
      "(100, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3370118000020739\n",
      "(100, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3413400999997975\n",
      "(100, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3001385000025039\n",
      "(100, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3491367000024184\n",
      "(100, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3230317999987165\n",
      "(1000, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.279745500003628\n",
      "(1000, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3476896000065608\n",
      "(1000, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.380329299994628\n",
      "(1000, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.3010429999994813\n",
      "(1000, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:00<00:01,  1.82ba/s]"
     ]
    }
   ],
   "source": [
    "#Define our examination parameters\n",
    "\n",
    "n_batches = [100, 1000, 2000]\n",
    "batch_sizes = [2, 4, 8, 16]\n",
    "tokenizers = [SentencePieceBPETokenizer,\n",
    "              CharBPETokenizer,\n",
    "              SentencePieceUnigramTokenizer,\n",
    "              ByteLevelBPETokenizer,\n",
    "              BertWordPieceTokenizer,\n",
    "              ]\n",
    "lengths = [[0, 5000], [5000, 10000], [10000, 15000]]\n",
    "options = product(n_batches, batch_sizes, tokenizers, lengths)\n",
    "\n",
    "#Train tokenizers\n",
    "statistics = []\n",
    "for option in options:\n",
    "    print(option)\n",
    "    statistics.append(train_tokenizer(training, *option))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}