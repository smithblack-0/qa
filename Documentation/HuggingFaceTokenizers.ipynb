{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tokenization\n",
    "\n",
    "This section will be related to an exploration of the huggingface\n",
    "tokenization word\n",
    "\n",
    "## Regex\n",
    "\n",
    "Notably, a lot of different features use regex. the regex feature\n",
    "is located in tokenizers as \"from tokenizers import Regex\"\n",
    "\n",
    "## Normalization\n",
    "\n",
    "Normalization, and normalizers, are functions\n",
    "responsible for translating and handling the cleanup\n",
    "of strings. They are responsible for such activities\n",
    "as separating concept-level languages, translating unicode\n",
    "characters into more meaningful forms, and other sorts\n",
    "of text refinement.\n",
    "\n",
    "Anything involving \"cleaning\" the text, in other words,\n",
    "should go here."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tokenizers import normalizers\n",
    "\n",
    "norms = [\n",
    "    normalizers.NFKD(),\n",
    "    normalizers.Strip(),\n",
    "    normalizers.StripAccents(),\n",
    "]\n",
    "normalizer = normalizers.Sequence(norms)\n",
    "normalizer.normalize_str(\"this is a test\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-Tokenization\n",
    "\n",
    "Pre-Tokenization is the process of splitting up the string into\n",
    "subcomponents which we will then be using for actual tokenization. It performs the \"split into arrays\" portion of tokenization.\n",
    "\n",
    "Importantly, pretokenization can be applied in sequence with absolutely no problem.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from tokenizers import pre_tokenizers\n",
    "\n",
    "#Standard tokenization\n",
    "pretoksequence = [\n",
    "    pre_tokenizers.Punctuation(),\n",
    "    pre_tokenizers.Whitespace(),\n",
    "    pre_tokenizers.Digits(individual_digits=True),\n",
    "\n",
    "]\n",
    "\n",
    "pretokenizer = pre_tokenizers.Sequence(pretoksequence)\n",
    "item = pretokenizer.pre_tokenize_str(\"Hey, let's see how this does: 123445\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "[('H', (0, 1)),\n ('e', (1, 2)),\n ('y', (2, 3)),\n (',', (3, 4)),\n (' ', (4, 5)),\n ('l', (5, 6)),\n ('e', (6, 7)),\n ('t', (7, 8)),\n (\"'\", (8, 9)),\n ('s', (9, 10)),\n (' ', (10, 11)),\n ('s', (11, 12)),\n ('e', (12, 13)),\n ('e', (13, 14)),\n (' ', (14, 15)),\n ('h', (15, 16)),\n ('o', (16, 17)),\n ('w', (17, 18)),\n (' ', (18, 19)),\n ('t', (19, 20)),\n ('h', (20, 21)),\n ('i', (21, 22)),\n ('s', (22, 23)),\n (' ', (23, 24)),\n ('d', (24, 25)),\n ('o', (25, 26)),\n ('e', (26, 27)),\n ('s', (27, 28)),\n (':', (28, 29)),\n (' ', (29, 30)),\n ('1', (30, 31)),\n ('2', (31, 32)),\n ('3', (32, 33)),\n ('4', (33, 34)),\n ('4', (34, 35)),\n ('5', (35, 36))]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Custom char pretokenizer. Note the usage of regex.\n",
    "#\n",
    "# The core library is written in rust, so this is faster\n",
    "#than a custom class.\n",
    "from tokenizers import Regex\n",
    "\n",
    "pattern = Regex('.')\n",
    "pretokenizer=  pre_tokenizers.Split(pattern, 'isolated')\n",
    "pretokenizer.pre_tokenize_str(\"Hey, let's see how this does: 123445\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n",
    "\n",
    "Any kind of tokenization requires training. At some point along the way, the tokenization system must construct a vocabulary of some kind, such that words can be turned into tokens and vice versa. The degree of training, however, varies.\n",
    "\n",
    "## WordLevel\n",
    "\n",
    "The most simple variety of tokenizer is perhaps the word level tokenizer. It functions by means of simply mapping the incoming tokens onto a vocabulary, whose size is limited. Unknown words must, by virtue of being unknown, be represented in terms of a unique unknown token, and the model makes its best guess as to the content of the token.\n",
    "\n",
    "## BPE, WordPiece\n",
    "\n",
    "Both of these build up their vocabulary in much the same way.\n",
    "\n",
    "Figure out all the individual characters which are within the input stream. Take those characters, and then look at the text. Figure out what groups of characters, when merged together, would reduce the overall token count. Repeat until vocabulary is within acceptable limits.\n",
    "\n",
    "Byte Pair Encoding and WordPiece are both excellent tokenization agorithms which require training. They look at the entire vocabulary, and attempt to merge it down to a particular size.\n",
    "\n",
    "## Unigram\n",
    "\n",
    "Unigram does something else. It starts by initializing a complete vocabulary, and then prunes down from here.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "% md\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extra\n",
    "\n",
    "A few prebuilt, additional tokenizers are available. They still require training\n",
    "\n",
    "## Sentencepiece.\n",
    "\n",
    "Sentencepiece works well when a language does not have a clear distinguishment between words, as in, for example, chinese. It treats an entire sentence as a token, and then attempts to break it apart into bits\n",
    "\n",
    "\n",
    "The two varieties are\n",
    "\n",
    "* SentencePieceUnigramTokenizer\n",
    "* SentencePieceBPETokenizer,\n",
    "\n",
    "Due to the way imports are run, pycharm, and likely other ide's, do not detect thes eas valid imports before running\n",
    "\n",
    "## Extra\n",
    "\n",
    "A few extra tokenizers are available, but perhaps not clearly listed\n",
    "\n",
    "* CharBPETokenizer\n",
    "* ByteLevelBPETokenizer\n",
    "* BertWordPieceTokenizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a basic pretrained tokenizer.\n",
    "\n",
    "Training a tokenizer takes quite a bit of time, but is nonetheless fairly straightforward. Make a dataset of the text content, shove it through an iterator, and let it work from there. One additional item of note: It is likely beneficial to start from a premade tokenizer which is reasonably close to your application.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": "['Test', 'this', 'token', '##ization', '119', '##0']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "import transformers\n",
    "\n",
    "## Load a premade tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer.tokenize(\"Test this tokenization 1190\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 2662/2662 [00:00<00:00, 14921.24it/s]\n",
      "Using custom data configuration train-novels-042faceeba55f51c\n",
      "Reusing dataset text (C:\\Users\\chris\\.cache\\huggingface\\datasets\\text\\train-novels-042faceeba55f51c\\0.0.0\\acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08)\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 14747362\n    })\n})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastai.text.all import get_text_files\n",
    "import datasets\n",
    "\n",
    "\n",
    "#Update it to be more suitable to the particular application using more training.\n",
    "#Make sure to use this to extract data. Significantly faster than raw python\n",
    "\n",
    "src = r'C:\\Users\\chris\\PycharmProjects\\qa\\Data\\lambada-dataset\\train-novels'\n",
    "files = [str(item) for item in get_text_files(src)]\n",
    "ds = datasets.load_dataset(src, '.txt', data_files=files )\n",
    "ds"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': [\" prism story of ci prism -lrb- story of ci -rrb- copyright 2010 rachel moschell published by rachel moschell at smashwords third edition the prism table of contents 0 gilded 1 purple 2 gaudy gold 3 electric blue 4 mocha 5 aquamarine 6 emerald 7 white plaster 8 scarlet 9 silver 10 midnight blue 11 coffee 12 hazel 13 olive green 14 beet red 15 pale 16 canary yellow 17 brick red 18 dark 19 transparent 20 red white and blue 21 cinnamon 22 sickly pink 23 bittersweet 24 sea green 25 crimson 26 pale blue 27 sapphire 28 fiery 29 turquoise 30 blond 31 plaid 32 white 33 grape 34 lilac preview of reverb -lrb- story of ci 2 -rrb- 0 gilded the silvery branches of the molle tree whispered in the shade , sprinkling soft leaves in the dirt of the boy 's path . \",\n  ' the sky shone sapphire behind the lacy branches , and the only other life along this back road were two tawny cows grazing in the silver white grass of the ditch . ',\n  ' behind them ran the same crumbling adobe wall the boy saw every day as he walked this way out of town for the afternoon session at the christian school his parents sent him to . ',\n  ' today was tuesday , and that meant bible class . ',\n  ' in one coffee-colored hand , the boy clutched his fat black bible with gilded letters , a present from his parents on his last birthday , fourteen . ',\n  ' he shuffled slowly along the road , feeling the bible burn into his fingertips , trying not to free the furious tears scalding the back of his eyes . ',\n  ' he should leave the bible here in the ditch by the cows . ',\n  ' because now the boy knew : none of it really mattered . ',\n  ' oh , he was quite sure there was justice and virtue in the world , and he intended to find it . ',\n  ' but it was not here . ']}"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "training = ds['train']\n",
    "training.shuffle()\n",
    "training[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batches: 10000\n",
      "batch 0 of 10000\n",
      "batch 1000 of 10000\n",
      "batch 2000 of 10000\n",
      "batch 3000 of 10000\n",
      "batch 4000 of 10000\n",
      "batch 5000 of 10000\n",
      "batch 6000 of 10000\n",
      "batch 7000 of 10000\n",
      "batch 8000 of 10000\n",
      "batch 9000 of 10000\n",
      "batch 10000 of 10000\n"
     ]
    }
   ],
   "source": [
    "#Create a generator to load batches of data.\n",
    "\n",
    "batch_size = 128\n",
    "def loader(ds_split, max_batches = 10000):\n",
    "    total_batches = min(len(ds_split) // batch_size + 1, max_batches)\n",
    "    batch_count = 0\n",
    "    print(\"total batches: %s\" % total_batches)\n",
    "    collection = []\n",
    "\n",
    "    for i, instance in enumerate(ds_split):\n",
    "        collection.append(instance['text'])\n",
    "        if len(collection) > batch_size:\n",
    "            if batch_count % 1000 == 0:\n",
    "                print(\"batch %s of %s\" % (batch_count, total_batches))\n",
    "            yield collection\n",
    "            batch_count += 1\n",
    "            collection = []\n",
    "        if batch_count > max_batches:\n",
    "            break\n",
    "    yield collection\n",
    "\n",
    "#Train the data based on the vocab\n",
    "ds_loader = loader(training)\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(ds_loader, vocab_size=60000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "('my_example_pretrained\\\\tokenizer_config.json',\n 'my_example_pretrained\\\\special_tokens_map.json',\n 'my_example_pretrained\\\\vocab.txt',\n 'my_example_pretrained\\\\added_tokens.json',\n 'my_example_pretrained\\\\tokenizer.json')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Once trained, you want to save it so you can use it again later\n",
    "new_tokenizer.save_pretrained(\"my_example_pretrained\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " they were in position , ten feet away from their home ship . \n"
     ]
    },
    {
     "data": {
      "text/plain": "['they',\n 'were',\n 'in',\n 'position',\n ',',\n 'ten',\n 'feet',\n 'away',\n 'from',\n 'their',\n 'home',\n 'ship',\n '.']"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And it can now be loaded from here!\n",
    "import random\n",
    "my_tokenizer = transformers.AutoTokenizer.from_pretrained(\"my_example_pretrained\")\n",
    "example = training[random.randint(0, 1000000)]['text']\n",
    "print(example)\n",
    "my_tokenizer.tokenize(example)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training tokenizers completely from scratch: A comparison\n",
    "\n",
    "Sometimes, it might become necessary to train a tokenizer completely from scratch. In which case, it is possible to initialize a raw tokenizer, then go use the train iterator.\n",
    "\n",
    "Lets train a few different tokenizers from scratch. We will keep track of the time for comparison, and only train them briefly. We will train, basically, all of them. These are all basically predefined pipelines.\n",
    "\n",
    "* SentencePieceBPETokenizer\n",
    "*\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "Tokenizer(vocabulary_size=0, model=SentencePieceBPE, unk_token=<unk>, replacement=▁, add_prefix_space=True, dropout=None)"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run imports\n",
    "import datasets\n",
    "import timeit\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "\n",
    "#Get tokenizers\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from tokenizers import CharBPETokenizer\n",
    "from tokenizers import SentencePieceUnigramTokenizer\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "SentencePieceBPETokenizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "\"<class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>\""
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset lambada (C:\\Users\\chris\\.cache\\huggingface\\datasets\\lambada\\plain_text\\1.1.0\\e32d76a7236c9ebb30099bc73d677c3acf32ddffb411836fe9ffc091ad3f3bec)\n",
      "100%|██████████| 3/3 [00:00<00:00, 25.88it/s]\n"
     ]
    }
   ],
   "source": [
    "#Configure our dataset\n",
    "\n",
    "ds = datasets.load_dataset(\"lambada\")\n",
    "training = ds['train']\n",
    "\n",
    "def filter_length(x, min_length, max_length):\n",
    "    return min_length < len(x['text']) < max_length\n",
    "\n",
    "\n",
    "def yield_batches(dataset: datasets.Dataset,\n",
    "                 min_length: int,\n",
    "                 max_length:int,\n",
    "                 batch_size: int,\n",
    "                 num_batches: int):\n",
    "    \"\"\"\n",
    "\n",
    "    Gets the data for training\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = dataset.filter(partial(filter_length, min_length=min_length, max_length=max_length))\n",
    "    batch_collection = []\n",
    "    batch_num = 0\n",
    "    for item in dataset:\n",
    "        batch_collection.append(item['text'])\n",
    "        if len(batch_collection) >= batch_size:\n",
    "            if batch_num % (num_batches//10) == 0:\n",
    "                print(\"batch %s of %s\" % (batch_num, num_batches))\n",
    "\n",
    "            yield batch_collection\n",
    "            if batch_num >= num_batches:\n",
    "                break\n",
    "            batch_collection = []\n",
    "            batch_num += 1\n",
    "    yield batch_collection\n",
    "\n",
    "\n",
    "def train_tokenizer(dset, n_batches, batch_size, tokenizer, lengths):\n",
    "    print(\"training tokenizer: number %s, batch %s, token %s, lengths %s\"\n",
    "          % (n_batches, batch_size, str(tokenizer), lengths))\n",
    "    statistics = {\n",
    "        'n_batches' : n_batches,\n",
    "        'batch_size' : batch_size,\n",
    "        'tokenizer' : str(tokenizer)}\n",
    "\n",
    "    train_batches = yield_batches(dset, lengths[0], lengths[1], batch_size, n_batches)\n",
    "    tokenizer = tokenizer()\n",
    "    start = timer()\n",
    "\n",
    "    #Note it is \"Train From Iterator\" not \"Train_new_from_iterator\"\n",
    "    tokenizer = tokenizer.train_from_iterator(train_batches)\n",
    "    end = timer()\n",
    "    print(\"elapsed %s\" % (end-start))\n",
    "    statistics['elapsed'] = end-start\n",
    "    statistics['tkn'] = tokenizer\n",
    "    return statistics\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.05ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.4698666999975103\n",
      "(100, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.47ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2626090000048862\n",
      "(100, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.402710099995602\n",
      "(100, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.308307799998147\n",
      "(100, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3036624999949709\n",
      "(100, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.4062673000007635\n",
      "(100, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.37ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.290019799998845\n",
      "(100, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2781465000007302\n",
      "(100, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.3040133999966201\n",
      "(100, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.31438640000124\n",
      "(100, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3342517999990378\n",
      "(100, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.32ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.380697799999325\n",
      "(100, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.18ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.3851044999973965\n",
      "(100, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2812413000065135\n",
      "(100, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 100\n",
      "elapsed 1.370277599999099\n",
      "(100, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3553107999978238\n",
      "(100, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.41ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2875274999969406\n",
      "(100, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3693955999988248\n",
      "(100, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3483900999999605\n",
      "(100, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.34861830000591\n",
      "(100, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3666929000028176\n",
      "(100, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.22ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3773930000024848\n",
      "(100, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2726894000006723\n",
      "(100, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3240248000001884\n",
      "(100, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3045899999997346\n",
      "(100, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.285140500003763\n",
      "(100, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3531015000044135\n",
      "(100, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.344855399998778\n",
      "(100, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3047922999976436\n",
      "(100, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.41ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3256593999976758\n",
      "(100, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3207915000020876\n",
      "(100, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3227706000034232\n",
      "(100, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.37ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.333167400000093\n",
      "(100, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.26ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3368475000024773\n",
      "(100, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3059544999996433\n",
      "(100, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.333754699997371\n",
      "(100, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.26ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3554980000044452\n",
      "(100, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3100906999970903\n",
      "(100, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3590234000002965\n",
      "(100, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3260941999978968\n",
      "(100, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.324911499999871\n",
      "(100, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3758159999997588\n",
      "(100, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.345412499998929\n",
      "(100, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3096148000040557\n",
      "(100, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3352030999958515\n",
      "(100, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.23ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.5889750999995158\n",
      "(100, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3605236999937915\n",
      "(100, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.22ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.434138599994185\n",
      "(100, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2925938999978825\n",
      "(100, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.30ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3456388999984483\n",
      "(100, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.17ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.449639800004661\n",
      "(100, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.37ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.285780699996394\n",
      "(100, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3143271999942954\n",
      "(100, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3985222999981488\n",
      "(100, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2947859000050812\n",
      "(100, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2896083999949042\n",
      "(100, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3764359000051627\n",
      "(100, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.292035800004669\n",
      "(100, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.297579999998561\n",
      "(100, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4136098000017228\n",
      "(100, 128, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.41ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2645817999946303\n",
      "(100, 128, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.322761300005368\n",
      "(100, 128, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.22ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4395151999997324\n",
      "(100, 128, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2478858000031323\n",
      "(100, 128, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2874827999985428\n",
      "(100, 128, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3749468000023626\n",
      "(100, 128, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3033780999976443\n",
      "(100, 128, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3416257000062615\n",
      "(100, 128, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4182228999998188\n",
      "(100, 128, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2770560999997542\n",
      "(100, 128, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.45ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2768819999982952\n",
      "(100, 128, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3802147999958834\n",
      "(100, 128, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.320207600001595\n",
      "(100, 128, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3086283999946318\n",
      "(100, 128, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 128, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.391110700002173\n",
      "(100, 512, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2866006000040215\n",
      "(100, 512, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.27309989999776\n",
      "(100, 512, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3955330000026152\n",
      "(100, 512, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3189144999996643\n",
      "(100, 512, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.300865099998191\n",
      "(100, 512, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.26ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3900425999963772\n",
      "(100, 512, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.37ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2948443999994197\n",
      "(100, 512, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3011882999999216\n",
      "(100, 512, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.412812100003066\n",
      "(100, 512, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.20ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3812095999965095\n",
      "(100, 512, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.19ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4220996000003652\n",
      "(100, 512, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.397398000000976\n",
      "(100, 512, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.09ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4547875999996904\n",
      "(100, 512, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.19ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4120791999957873\n",
      "(100, 512, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 100, batch 512, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.19ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4490174000020488\n",
      "(1000, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.52ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.200914399996691\n",
      "(1000, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.45ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2700600999960443\n",
      "(1000, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.43ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.3154764999999315\n",
      "(1000, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.254204100005154\n",
      "(1000, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.48ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2562134000036167\n",
      "(1000, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.3089976999981445\n",
      "(1000, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.49ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.224606899995706\n",
      "(1000, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2581924999976764\n",
      "(1000, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.52ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.2699780999973882\n",
      "(1000, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.292073500000697\n",
      "(1000, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.45ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2716184000018984\n",
      "(1000, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.3101387999995495\n",
      "(1000, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.43ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.2514480000027106\n",
      "(1000, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3055865000060294\n",
      "(1000, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 1000\n",
      "elapsed 1.3239897000021301\n",
      "(1000, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.51ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2133666000008816\n",
      "(1000, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2894097000025795\n",
      "(1000, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3178912999937893\n",
      "(1000, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2504961000013282\n",
      "(1000, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2996713999964413\n",
      "(1000, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.45ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2914640000017243\n",
      "(1000, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2377999000018463\n",
      "(1000, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2937950999985333\n",
      "(1000, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.41ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3211502000049222\n",
      "(1000, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2664313999994192\n",
      "(1000, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.279356099999859\n",
      "(1000, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.49ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2900277000007918\n",
      "(1000, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2912522999977227\n",
      "(1000, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2906505999999354\n",
      "(1000, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.349055500002578\n",
      "(1000, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2708974999986822\n",
      "(1000, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3331260999984806\n",
      "(1000, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.45ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3106248999974923\n",
      "(1000, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3033273999972153\n",
      "(1000, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3661830000055488\n",
      "(1000, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2855498999997508\n",
      "(1000, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.53ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2152356000005966\n",
      "(1000, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.333075499998813\n",
      "(1000, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.314418800000567\n",
      "(1000, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.47ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.227751200000057\n",
      "(1000, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3303415999980643\n",
      "(1000, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.331898700002057\n",
      "(1000, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2676386000021012\n",
      "(1000, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2736834999959683\n",
      "(1000, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3048737999997684\n",
      "(1000, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.272738700004993\n",
      "(1000, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2925355000043055\n",
      "(1000, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3182401999947615\n",
      "(1000, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.41ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2520533999995678\n",
      "(1000, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2581883000020753\n",
      "(1000, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3406093000012334\n",
      "(1000, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.45ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2483636000033584\n",
      "(1000, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2688587000011466\n",
      "(1000, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3368304000032367\n",
      "(1000, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2952283000049647\n",
      "(1000, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2610511999955634\n",
      "(1000, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3699772000036319\n",
      "(1000, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.21ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3796446999986074\n",
      "(1000, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2708786000002874\n",
      "(1000, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.49ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2660964999959106\n",
      "(1000, 128, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3571021000025212\n",
      "(1000, 128, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.12ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4524744000009377\n",
      "(1000, 128, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4048234000001685\n",
      "(1000, 128, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2922557000056258\n",
      "(1000, 128, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.45ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.25656980000349\n",
      "(1000, 128, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3027540999974008\n",
      "(1000, 128, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.319533900001261\n",
      "(1000, 128, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2894654999981867\n",
      "(1000, 128, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3020045999946888\n",
      "(1000, 128, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.307457599999907\n",
      "(1000, 128, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3251754000011715\n",
      "(1000, 128, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.351829599996563\n",
      "(1000, 128, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.32ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.314896800002316\n",
      "(1000, 128, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.358013299999584\n",
      "(1000, 128, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 128, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3118658000021242\n",
      "(1000, 512, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3204022999998415\n",
      "(1000, 512, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3180799999972805\n",
      "(1000, 512, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2952651000014157\n",
      "(1000, 512, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2553640999976778\n",
      "(1000, 512, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3448980000030133\n",
      "(1000, 512, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.48ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2873660000041127\n",
      "(1000, 512, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2567354999991949\n",
      "(1000, 512, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3324554999999236\n",
      "(1000, 512, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2970981999969808\n",
      "(1000, 512, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2490162000030978\n",
      "(1000, 512, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.47ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2681501999977627\n",
      "(1000, 512, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.350388099999691\n",
      "(1000, 512, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.43ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2539929999984452\n",
      "(1000, 512, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3114386999950511\n",
      "(1000, 512, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 1000, batch 512, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3456772999998066\n",
      "(5000, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 5000\n",
      "elapsed 1.329110300001048\n",
      "(5000, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.50ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2390035999997053\n",
      "(5000, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 5000\n",
      "elapsed 1.3590739999999641\n",
      "(5000, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 5000\n",
      "elapsed 1.292099899997993\n",
      "(5000, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.45ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2652719999969122\n",
      "(5000, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 5000\n",
      "elapsed 1.3505603000012343\n",
      "(5000, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 5000\n",
      "elapsed 1.299893900002644\n",
      "(5000, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.46ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.264629800003604\n",
      "(5000, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.48ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 5000\n",
      "elapsed 1.2824107999986154\n",
      "(5000, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 5000\n",
      "elapsed 1.3249858999988646\n",
      "(5000, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.37ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3186052999953972\n",
      "(5000, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 5000\n",
      "elapsed 1.4121297000019695\n",
      "(5000, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.02ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 5000\n",
      "elapsed 1.4993319999994128\n",
      "(5000, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.18ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4238438999964274\n",
      "(5000, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.21ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 5000\n",
      "elapsed 1.434465899998031\n",
      "(5000, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3082035000043106\n",
      "(5000, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.10ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4839636000033352\n",
      "(5000, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3623833999954513\n",
      "(5000, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2953351000032853\n",
      "(5000, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3873683999991044\n",
      "(5000, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3739287000062177\n",
      "(5000, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.37ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.28705269999773\n",
      "(5000, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.95ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.598878800003149\n",
      "(5000, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.17ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4970352000018465\n",
      "(5000, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.93ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.6180222000039066\n",
      "(5000, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.14ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.45802150000236\n",
      "(5000, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.14ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.5017280999963987\n",
      "(5000, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3551173999949242\n",
      "(5000, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.379759200004628\n",
      "(5000, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.23ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4362769999934244\n",
      "(5000, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.08ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4607130000003963\n",
      "(5000, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.11ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4628260000026785\n",
      "(5000, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.30ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4112865999995847\n",
      "(5000, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2949411000008695\n",
      "(5000, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3577501999970991\n",
      "(5000, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.22ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4367186999952537\n",
      "(5000, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.317804199999955\n",
      "(5000, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3498419000025024\n",
      "(5000, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.20ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4418108999961987\n",
      "(5000, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2569395000027725\n",
      "(5000, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.13ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.479823600006057\n",
      "(5000, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4349426999979187\n",
      "(5000, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.273845399999118\n",
      "(5000, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3738346000027377\n",
      "(5000, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.18ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4578916000027675\n",
      "(5000, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.41ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.25376729999698\n",
      "(5000, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.329052100001718\n",
      "(5000, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.358776000000944\n",
      "(5000, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.310626599995885\n",
      "(5000, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.369862400002603\n",
      "(5000, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3389246999940951\n",
      "(5000, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2999700000000303\n",
      "(5000, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3615768000017852\n",
      "(5000, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3290689999994356\n",
      "(5000, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2786641000056989\n",
      "(5000, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.360210899998492\n",
      "(5000, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3279500999997254\n",
      "(5000, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.40ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2663689000037266\n",
      "(5000, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.20ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4099415999953635\n",
      "(5000, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.37ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3348925999962375\n",
      "(5000, 128, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2741964000015287\n",
      "(5000, 128, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.326464500001748\n",
      "(5000, 128, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.350051599998551\n",
      "(5000, 128, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.14ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4118308999968576\n",
      "(5000, 128, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3671210000029532\n",
      "(5000, 128, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3934253999977955\n",
      "(5000, 128, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.16ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4142405999955372\n",
      "(5000, 128, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.16ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4364686999979313\n",
      "(5000, 128, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3890929999979562\n",
      "(5000, 128, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.22ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3699290000004112\n",
      "(5000, 128, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3705952999953297\n",
      "(5000, 128, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.14ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4874221999998554\n",
      "(5000, 128, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3168287000007695\n",
      "(5000, 128, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3315935999999056\n",
      "(5000, 128, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 128, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.18ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4627046000023256\n",
      "(5000, 512, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.274149299999408\n",
      "(5000, 512, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3845269000012195\n",
      "(5000, 512, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3872714999961318\n",
      "(5000, 512, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.13ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.422992499996326\n",
      "(5000, 512, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.88ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.6405472999977064\n",
      "(5000, 512, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.22ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4402187000014237\n",
      "(5000, 512, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.03ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4975778999942122\n",
      "(5000, 512, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.04ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.5234302000026219\n",
      "(5000, 512, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.19ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.463088600001356\n",
      "(5000, 512, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4097698999976274\n",
      "(5000, 512, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3427956000014092\n",
      "(5000, 512, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.22ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4377601000014693\n",
      "(5000, 512, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.26ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3401593000016874\n",
      "(5000, 512, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.11ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4971432000020286\n",
      "(5000, 512, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 5000, batch 512, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.16ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.484722100001818\n",
      "(10000, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 10000\n",
      "elapsed 1.3219893999994383\n",
      "(10000, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.26ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3788956999997026\n",
      "(10000, 2, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.20ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 10000\n",
      "elapsed 1.4433292000030633\n",
      "(10000, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.13ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 10000\n",
      "elapsed 1.4466494000007515\n",
      "(10000, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3253198000020348\n",
      "(10000, 2, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.01ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 10000\n",
      "elapsed 1.6113498999984586\n",
      "(10000, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 10000\n",
      "elapsed 1.291017000003194\n",
      "(10000, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.19ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4138271999981953\n",
      "(10000, 2, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.26ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 10000\n",
      "elapsed 1.4124547000028542\n",
      "(10000, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 10000\n",
      "elapsed 1.3237083999993047\n",
      "(10000, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3568699000024935\n",
      "(10000, 2, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 10000\n",
      "elapsed 1.4118359000058263\n",
      "(10000, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.41ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 10000\n",
      "elapsed 1.2675487000014982\n",
      "(10000, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3787685999996029\n",
      "(10000, 2, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 2, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 of 10000\n",
      "elapsed 1.3859240999954636\n",
      "(10000, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.44ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2502753000007942\n",
      "(10000, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.17ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4356311000010464\n",
      "(10000, 4, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.16ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4718590999982553\n",
      "(10000, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.19ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3753908999933628\n",
      "(10000, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.21ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.410282400000142\n",
      "(10000, 4, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3640213999970001\n",
      "(10000, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.19ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3930880000043544\n",
      "(10000, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3697268999967491\n",
      "(10000, 4, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3639172999974107\n",
      "(10000, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3585166999982903\n",
      "(10000, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.12ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4721379999973578\n",
      "(10000, 4, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4303647999986424\n",
      "(10000, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.45ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2446373000057065\n",
      "(10000, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3619959999996354\n",
      "(10000, 4, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 4, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.26ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4091268999982276\n",
      "(10000, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3080265000025975\n",
      "(10000, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3783019999973476\n",
      "(10000, 8, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4023556000029203\n",
      "(10000, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.22ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3710253000026569\n",
      "(10000, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.23ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3923064999980852\n",
      "(10000, 8, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3350793000063277\n",
      "(10000, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3663742000062484\n",
      "(10000, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.18ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.420347600003879\n",
      "(10000, 8, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.30ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3816522999986773\n",
      "(10000, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.19ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.389831000000413\n",
      "(10000, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.23ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3987832999991952\n",
      "(10000, 8, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.388585200002126\n",
      "(10000, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.26ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3477109000014025\n",
      "(10000, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.32ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3414534000039566\n",
      "(10000, 8, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 8, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.418507000002137\n",
      "(10000, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.11ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4577980999965803\n",
      "(10000, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.13ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.455736000003526\n",
      "(10000, 16, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.03ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.5730136999991373\n",
      "(10000, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.11ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.438917899999069\n",
      "(10000, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.14ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4471553999974276\n",
      "(10000, 16, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.14ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.478328600001987\n",
      "(10000, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.20ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.388844600005541\n",
      "(10000, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.30ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.35809860000154\n",
      "(10000, 16, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4194032000013976\n",
      "(10000, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.22ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3684047999995528\n",
      "(10000, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3937700000024051\n",
      "(10000, 16, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4390150000035646\n",
      "(10000, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3337009999959264\n",
      "(10000, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.32ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3595730000015465\n",
      "(10000, 16, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 16, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.378846400002658\n",
      "(10000, 128, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.32ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3125007999988156\n",
      "(10000, 128, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.26ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3781693000055384\n",
      "(10000, 128, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.25ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4146165999991354\n",
      "(10000, 128, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.11ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4389564000011887\n",
      "(10000, 128, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.19ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4120239999974729\n",
      "(10000, 128, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3440297999986797\n",
      "(10000, 128, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2911983999947552\n",
      "(10000, 128, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.36539410000114\n",
      "(10000, 128, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.314383999997517\n",
      "(10000, 128, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.29ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.328509099999792\n",
      "(10000, 128, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.27ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.371450200000254\n",
      "(10000, 128, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3081891000038013\n",
      "(10000, 128, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.23ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.371899999998277\n",
      "(10000, 128, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.33ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3385811999978614\n",
      "(10000, 128, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 128, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3145698000007542\n",
      "(10000, 512, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2729987999991863\n",
      "(10000, 512, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3834779000026174\n",
      "(10000, 512, <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.sentencepiece_bpe.SentencePieceBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.340383800001291\n",
      "(10000, 512, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.35ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2887613000057172\n",
      "(10000, 512, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.24ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3865051999964635\n",
      "(10000, 512, <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.char_level_bpe.CharBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.31ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3624775000062073\n",
      "(10000, 512, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.39ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.2783521999954246\n",
      "(10000, 512, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.19ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.4055128000036348\n",
      "(10000, 512, <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.sentencepiece_unigram.SentencePieceUnigramTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.34ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3574027000067872\n",
      "(10000, 512, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.28ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3471714999977848\n",
      "(10000, 512, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.26ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.382874199996877\n",
      "(10000, 512, <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.byte_level_bpe.ByteLevelBPETokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.36ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3268754999953671\n",
      "(10000, 512, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [0, 5000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [0, 5000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.23ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3636760000008508\n",
      "(10000, 512, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [5000, 10000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [5000, 10000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.38ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3089316000041435\n",
      "(10000, 512, <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, [10000, 15000])\n",
      "training tokenizer: number 10000, batch 512, token <class 'tokenizers.implementations.bert_wordpiece.BertWordPieceTokenizer'>, lengths [10000, 15000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.41ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.3253727999981493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Define our examination parameters\n",
    "\n",
    "n_batches = [100, 1000, 5000, 10000]\n",
    "batch_sizes = [2, 4, 8, 16, 128, 512]\n",
    "tokenizers = [SentencePieceBPETokenizer,\n",
    "              CharBPETokenizer,\n",
    "              SentencePieceUnigramTokenizer,\n",
    "              ByteLevelBPETokenizer,\n",
    "              BertWordPieceTokenizer,\n",
    "              ]\n",
    "lengths = [[0, 5000], [5000, 10000], [10000, 15000]]\n",
    "options = product(n_batches, batch_sizes, tokenizers, lengths)\n",
    "\n",
    "#Train tokenizers\n",
    "statistics = []\n",
    "for option in options:\n",
    "    print(option)\n",
    "    statistics.append(train_tokenizer(training, *option))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "     n_batches  batch_size                    tokenizer   elapsed\n0          100           2  SentencePieceBPETokenizer'>  1.469867\n1          100           2  SentencePieceBPETokenizer'>  1.262609\n2          100           2  SentencePieceBPETokenizer'>  1.402710\n3          100           2           CharBPETokenizer'>  1.308308\n4          100           2           CharBPETokenizer'>  1.303662\n..         ...         ...                          ...       ...\n355      10000         512      ByteLevelBPETokenizer'>  1.382874\n356      10000         512      ByteLevelBPETokenizer'>  1.326875\n357      10000         512     BertWordPieceTokenizer'>  1.363676\n358      10000         512     BertWordPieceTokenizer'>  1.308932\n359      10000         512     BertWordPieceTokenizer'>  1.325373\n\n[360 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_batches</th>\n      <th>batch_size</th>\n      <th>tokenizer</th>\n      <th>elapsed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>2</td>\n      <td>SentencePieceBPETokenizer'&gt;</td>\n      <td>1.469867</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>100</td>\n      <td>2</td>\n      <td>SentencePieceBPETokenizer'&gt;</td>\n      <td>1.262609</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100</td>\n      <td>2</td>\n      <td>SentencePieceBPETokenizer'&gt;</td>\n      <td>1.402710</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>100</td>\n      <td>2</td>\n      <td>CharBPETokenizer'&gt;</td>\n      <td>1.308308</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>100</td>\n      <td>2</td>\n      <td>CharBPETokenizer'&gt;</td>\n      <td>1.303662</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>355</th>\n      <td>10000</td>\n      <td>512</td>\n      <td>ByteLevelBPETokenizer'&gt;</td>\n      <td>1.382874</td>\n    </tr>\n    <tr>\n      <th>356</th>\n      <td>10000</td>\n      <td>512</td>\n      <td>ByteLevelBPETokenizer'&gt;</td>\n      <td>1.326875</td>\n    </tr>\n    <tr>\n      <th>357</th>\n      <td>10000</td>\n      <td>512</td>\n      <td>BertWordPieceTokenizer'&gt;</td>\n      <td>1.363676</td>\n    </tr>\n    <tr>\n      <th>358</th>\n      <td>10000</td>\n      <td>512</td>\n      <td>BertWordPieceTokenizer'&gt;</td>\n      <td>1.308932</td>\n    </tr>\n    <tr>\n      <th>359</th>\n      <td>10000</td>\n      <td>512</td>\n      <td>BertWordPieceTokenizer'&gt;</td>\n      <td>1.325373</td>\n    </tr>\n  </tbody>\n</table>\n<p>360 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze this a little with some pandas frames\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(statistics)\n",
    "df['tokenizer'] = df['tokenizer'].apply(lambda x : x.split(\".\")[-1])\n",
    "df = df.drop('tkn', axis=1)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 540x540 with 12 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Use seaborn to example behavior\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pg = sns.PairGrid(data=df)\n",
    "pg.map_diag(sns.histplot)\n",
    "pg.map_offdiag(sns.scatterplot)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}